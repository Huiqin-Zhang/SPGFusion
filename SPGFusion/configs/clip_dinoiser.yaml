_base_: "default.yml"
defaults:
  - _self_

seed: 0
model_name: CLIP_DINOiser
model:
  type: CLIP_DINOiser
  clip_model: ViT-B/16
  clip_model_pth: model/ViT-B-16-laion2B/open_clip_pytorch_model.bin
  clip_backbone: maskclip
  vit_arch: vit_base
  vit_patch_size: 16
  enc_type_feats: "v"
  gamma: 0.2
  in_dim: 256
  delta: 0.99
  img_size: 224
  patch_size: 16
  # feats_idx: final
  feats_idx: -3

train:
  batch_size: 16
  corr_lr: 0.00005
  found_lr: 0.01
  num_workers: 4
  epochs: 250
  milestones:
    - 8000
  step_lr_gamma: 0.1
  # data: "assets/ILSVRC2012_img_train_t3/train/vis/00341D.png" # CHANGE to your ImageNet data folder
  data: "assets" # CHANGE to your ImageNet data folder
  loss: CE
  ds_size:
  im_size: 448
# output: "assets/output" # CHANGE to your results folder
output: "assets" # CHANGE to your results folder

evaluate:
  eval_only: true
  task:
    - voc
    - context
    - coco_object
    - context59
    - voc20
    - coco_stuff
    - cityscapes
    - ade20k

  # evaluation
  voc: segmentation/configs/_base_/datasets/pascal_voc12.py
  voc20: segmentation/configs/_base_/datasets/pascal_voc12_20.py
  context: segmentation/configs/_base_/datasets/pascal_context.py
  context59: segmentation/configs/_base_/datasets/pascal_context59.py
  coco_stuff: segmentation/configs/_base_/datasets/stuff.py
  coco_object: segmentation/configs/_base_/datasets/coco.py
  cityscapes: segmentation/configs/_base_/datasets/cityscapes.py
  ade20k: segmentation/configs/_base_/datasets/ade20k.py
